# -*- coding: utf-8 -*-
"""PreprocesadoTextos_SuperFinalV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLrP4_llh1Hp_PGXnZIl1GGLHxBtyoZn

# Todas las cuestiones relativas al preprocesado figuran en este Collab

#### **Recordatorio**: aunque todo este preprocesado se haya hecho en grupo, se recuerda que la redacci√≥n ha de ser personalizada.

---
---

# Length Features

Aqu√≠ se recogen algunas variables relacionadas con la longitud de la frase tanto a nivel de token como a nivel de caracter.
"""

import pandas as pd
train_dataframe = pd.read_csv("https://raw.githubusercontent.com/jibt1/competition_group/main/Tareas/datasets/haha_2021_train.csv", sep=',')

train_dataframe.columns

text = train_dataframe[['id', 'text', 'is_humor', 'votes_no', 'votes_1', 'votes_2', 'votes_3',
       'votes_4', 'votes_5', 'humor_rating', 'humor_mechanism',
       'humor_target']]
#text = text.iloc[4650:4660,:] # comentar esta

def len_token(tweet):
  tweet_tokens = tweet.replace('\n', ' ').split(' ')
  return len(tweet_tokens)
def n_characters_tweet(tweet):
  tweet = tweet.replace('\n', ' ')
  return len(tweet)

def get_len_token(text):
  return list(map(len_token, text['text'].to_list()))

def get_n_characters_tweet(text):
  return list(map(n_characters_tweet, text['text'].to_list()))

def get_n_characters_tweet(text):
  return list(map(n_characters_tweet, text['text'].to_list()))

text['len_token'] = get_len_token(text)

text['n_char'] = get_n_characters_tweet(text)

text['ratio'] = text['len_token']/text['n_char']

text

"""---
---

# Word Emoji Context Matrix. ü§£ üòÜ üòÇ 

Descripci√≥n: Extracci√≥n/guardado de Emojis y depuraci√≥n del Corpus

Inconvenientes detectados con los emojis:

* 1 Resulta que los emojis hemos podido verificar que countvectorizer los ignora. Es decir si las frases fueran solo emojis el shape de la matriz word context ser√≠a (1,1), con valor empty.

* 2 Aun suponiendo que exista alguna alternativa a countvectorizer muchas personas escriben palabras juntas con emojis, e.g.: 'manüë®', lo que podr√≠a generar un token m√°s del vocabulario que innecesario. Adem√°s pueden escribir varios emojis juntos üë®, üë®üë®, üë®üë®üë®, ... generando tambi√©n un problema en su identificaci√≥n.

La propuesta ha sido realizar un preproceso como el siguiente. Adem√°s, para solucionar el problema de los emoticonos, en lugar de pas√°rselos a countvectorizer en bruto utilizamos la decodificaci√≥n a texto de la librer√≠a emoji: **emoji.demojize()**
"""

# %%capture
!pip install emoji

import emoji

def extract_emojis(tweet):
  return ''.join(c for c in tweet if c in emoji.UNICODE_EMOJI_ENGLISH)

def get_emojilist(tweet):
  emojistring = extract_emojis(tweet)
  emojilist = [emoji for emoji in emojistring]
  for emoji in emojilist:
    tweet = tweet.replace(emoji, "")
  return tweet, emojilist

def get_emoji_sentence(tweet):
  tweet, emojilist = get_emojilist(tweet)
  emoji_sentence = emoji.demojize(' '.join(emojilist))
  return tweet, emoji_sentence

def filter(text_df):
  filtered_df = pd.DataFrame(map(get_emoji_sentence, text_df['text'].to_list()))
  filtered_df.rename(columns={0: 'sentences_without_emojis', 1: 'emoji_sentences'}, inplace=True)
  return filtered_df

df_filter_output = filter(text)

df_cleaned_sentences = df_filter_output[['sentences_without_emojis']]
# df_cleaned_sentences = df_filter_output['emoji_sentences']

text['text_v2'] = df_filter_output['sentences_without_emojis'].to_list()

emoji_sentences = df_filter_output['emoji_sentences']

text['text_emojis'] = emoji_sentences.to_list()


def get_emoji_vocab(emoji_sentence):
  analyzer = CountVectorizer(binary=False, analyzer='word', # stop_words=more_stop_words,
                              ngram_range=(1, 1)).build_analyzer()
  return (emoji for emoji in analyzer(emoji_sentence))

from sklearn.feature_extraction.text import  CountVectorizer
cv_emoji = CountVectorizer(analyzer=get_emoji_vocab)

try:
  word_context_emoji = cv_emoji.fit_transform(emoji_sentences.to_list())
# Just if there is no emoji
except ValueError:
  emoji_sentences.loc[0,0] = emoji.demojize("üÜò")
  word_context_emoji = cv_emoji.fit_transform(emoji_sentences.to_list())
  """emoji_sentences_test = emoji_sentences"""
  """emoji_sentences_test.loc[0,0] = "üÜòüÜòüÜòüÜòüÜòüÜòüÜòüÜò"""
  """text['text_emojis'] = emoji_sentences_test.to_list()"""
# cv_emoji.get_feature_names()

cv_emoji.get_feature_names()

text

"""---
---

# Detector de cansinos y correcci√≥n de repeticiones innecesarias

En twitter hay mucho "pesao" con el pulgar "cansao" y lo dejan mucho tiempo en el m√≥ooooooooovil pulsando una letraaaaa. A todos estos, los consideramos unos pesaos y lo vamos a tener en cuenta. No quieren trabajar.

¬øPor qu√© corregiremos tambi√©n a los pesaos? Porque spellchecker no puede, es f√°cil ver esto con un ejemplo en c√≥digo.
"""

def is_cansino(tweet, tolerancia:int=2):
  # Consideramos pesados a todos aquellos
  # que repitan 3 veces una letra
  """
  Esta funci√≥n identifica a los pesaos
  Args:
    tolerancia (int): tolerancia al pesao
  """
  l1 = ""
  rep = 0
  letra = ""
  tweet_aux = tweet + '<eos>'
  for index, letra in enumerate(tweet_aux):
      if letra == l1:
          rep += 1
      elif rep >= tolerancia:
          return l1, rep, tolerancia, 1*True
      else:
        rep = 0
        l1 = letra
  return letra, 0, tolerancia, 1*False

def get_cansinos(corpus):
  L = list(map(is_cansino, corpus))
  return [ret[3] for ret in L]

###################################√á
##### Correccion repeticiones #####
###################################

# Esto es beta solo corrige la primera
# tanda de repeticiones se puede mejorar

def tweet_corrector(tweet):
    letra, rep, tolerancia, bool_ = is_cansino(tweet)
    if bool_ == 1:
        tweet = tweet.replace(rep*letra, '')
        tweet = tweet_corrector(tweet)
    return tweet

def corpus_corrector(corpus):
    L = list(map(tweet_corrector, corpus))
    return L

"""
text.loc[7,'text_v2'] = 'Meeee aaaaabuuuuurroooooo'
text
"""

text['detected_text_cansinos'] = get_cansinos(text['text_v2'].to_list())
text['detected_emojis_cansinos'] = get_cansinos(text['text_emojis'].to_list())
text['detected_cansinos'] = (text['detected_text_cansinos'] + text['detected_emojis_cansinos']) % 2
# text['detected_emojis_cansinos] | text['detected_text_cansinos'] 

text['text_v3'] = corpus_corrector(text['text_v2'].to_list())

text

"""---
---

# SpellChecker (Corrector ortogr√°fico)
"""

# SpellChecker creo que no hace falta, pero por si acaso la dejo
# !pip install SpellChecker

# pyspellchecker si hace falta instalarla
# %%capture
!pip install pyspellchecker
from spellchecker import SpellChecker

# Con distance=1 el corrector es m√°s conservador 
spell_check = SpellChecker(language='es', distance=1) # Gracias Yoan
# spell_check.correction("-")
# (9spell_check  = SpellChecker(language='es', distance=100)
# spell_check.correction("chicxs c√≥mo hestais")

string = 'hola'
'a' in string

def corrector_ortografico(tweet):
  tweet = tweet.replace('?', ' ? ')
  tweet = tweet.replace('¬ø', ' ¬ø ')
  tweet = tweet.replace('...', ' . ')
  tweet = tweet.replace('.', ' . ')
  tweet = tweet.replace('‚Äî', ' . ')
  tweet = tweet.replace('. .', ' . ')
  tweet = tweet.replace('.  .', ' . ')
  tweet = tweet.replace('.   .', ' . ')
  
  tweet = tweet.replace('\n', '').split(' ')
  

  corrected_tweet = [spell_check.correction(token) if token not in ['', '-', '?', '¬ø', ] else token for token in tweet]

  return ' '.join(corrected_tweet)

def corrector_corpus(corpus):
  return list(map(corrector_ortografico, corpus))

text['text_v4'] = corrector_corpus(text['text_v3'])

text[['text_v3', 'text_v4']]

text['text_v4'].iloc[6]

"""---
---

# Obtenci√≥n de Stop Words
"""

# %%capture
!pip install stop_words

from stop_words import get_stop_words # Se a√±ade librer√≠a para obtener las stop_word de cualquier idioma
!wget "https://raw.githubusercontent.com/jibt1/competition_group/main/Tareas/datasets/more_stop_words.txt"

# Se obtienen las stop_words en espa√±ol
import pickle
with open("more_stop_words.txt", "rb") as f:
  list_test = pickle.load(f)

stop_words = get_stop_words("spanish")
more_stop_words = stop_words + list_test
more_stop_words.sort()

"""# ELIMINACI√ìN DE IDIOMAS ALEJADOS DEL ESPA√ëOL"""

!pip install spacy_langdetect

# frase de prueba
# text = '¬øHola c√≥mo ÿ≥ŸÖÿ®ÿ£ÿ≥ŸÉŸÖŸÇ est√°is? Ï¢ÖÌòÑÏïÑÏÉùÏùºÏ∂ïÌïòÌï¥'

import spacy
from spacy_langdetect import LanguageDetector

# sentence level language detection
dict_ = {}

nlp = spacy.load('en')
nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)

# Eliminamos palabras koreanas, vietnamitas, japonesas o √°rabes
L = ['ko', 'vi', 'ja', 'ar', 'zh']

print("v3: ", text['text_v3'].iloc[6])
print("\n\n v4: ", text['text_v4'].iloc[6])

df_cleaned_sentences_list = df_cleaned_sentences['sentences_without_emojis'].to_list()

df_cleaned_sentences_list = text['text_v4'].to_list()

for index, sentence in enumerate(df_cleaned_sentences_list):
  doc = nlp(sentence)
  for subsent in doc.sents:
    
    if subsent._.language['language'] in L:
      
      df_cleaned_sentences_list[index] = sentence.replace(subsent.text,'')
df_cleaned_sentences_list

text['text_v5'] = df_cleaned_sentences_list


text

"""---
---

# CREACION MATRIZ WORD CONTEXT DEL TEXTO DEPURADO (SIN EMOJIS y sin idiomas poco habituales)
"""

from nltk.stem.snowball import SpanishStemmer
def spanish_stemmer(sentence):
    stemmer = SpanishStemmer()
    analyzer = CountVectorizer(binary=False, analyzer='word', stop_words=more_stop_words,
                               ngram_range=(1, 1)).build_analyzer()
    return (stemmer.stem(word) for word in analyzer(sentence))
    
from sklearn.feature_extraction.text import CountVectorizer
c_vec = CountVectorizer(analyzer=spanish_stemmer, stop_words=more_stop_words, lowercase=True)
# tf_idf = TfidfTransformer(smooth_idf=True, use_idf=True)
c_vec_mat = c_vec.fit_transform(text['text_v5'])
# tf_idf_mat = tf_idf.fit_transform(c_vec_mat)
c_vec_mat.shape

df_c_vec = pd.DataFrame(c_vec_mat.toarray(), columns=c_vec.get_feature_names())
print(df_c_vec)

"""---
---

# Traducci√≥n Corpus

# Descripci√≥n:

Frente a la adversidad que supone que la gran mayor√≠a de modelos desarrollados en HuggingFace se encuentran en Ingl√©s, se ha optado por traducir el corpus al ingl√©s con el pipeline correspondiente. As√≠, podemos aprovecharlos para hacer Transfer Learning.

Puesto que la fama de los transformers les antecede, consideramos que el error de los mismos de cara a las traducciones resulta asumible.
"""

!pip install transformers
!pip install sentencepiece

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import AutoModelForSequenceClassification, pipeline

# Autotokenizador
tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-es-en")

# Elecci√≥n/Descarga del modelo Preentrenado
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-es-en")

def get_corpus_translation(corpus):
  from transformers import pipeline
  pline = pipeline("translation_es_to_en", model=model, tokenizer=tokenizer)

  def get_tweet_translation(tweet):

    tweet = tweet.lower()

    tweet = tweet.replace('.', ' <.> ')
    tweet = tweet.replace('...', ' . ')
    tweet = tweet.replace(' q ', ' que ')
    tweet = tweet.replace(' k ', ' que ')
    tweet = tweet.replace(' d ', ' de ')

    return pline(tweet)[0]['translation_text'] # punto <point>

  return list(map(get_tweet_translation, corpus['text_v5'].to_list()))

text['en_text'] = get_corpus_translation(text)

def recover_point(tweet):
  return tweet.replace('<.>', ' . ')

def recover_point_corpus(corpus):
  return list(map(recover_point, corpus))

text['en_text'] = recover_point_corpus(text['en_text'])

text = text[['id', 'text', 'text_v2', 'text_v3', 'text_v4', 'text_v5',
             'en_text', 'len_token', 'n_char', 'ratio', 'detected_cansinos',
             'detected_emojis_cansinos', 'detected_text_cansinos', 'text_emojis', 'is_humor',
             'votes_no', 'votes_1', 'votes_2', 'votes_3','votes_4', 'votes_5', 'humor_rating', 'humor_mechanism',
             'humor_target']]

text.replace({'text_emojis': {'': 'no_emojis'}}, inplace= True)

corpus = text.copy()

#corpus['text_emojis'][3]

corpus.to_csv('cleaned_corpus.csv', index=False, sep = ',')

corpus['text_emojis'].loc[4658]

test_corpus = pd.read_csv('cleaned_corpus.csv', header=0, sep=',')

test_corpus.tail(8)

test_corpus['text_emojis'][~test_corpus['text_emojis'].isna()]

test_corpus.iloc[6]['text']

test_corpus.iloc[6]['text_v3']

test_corpus.iloc[6]['text_v4']

test_corpus.iloc[6]['text_v5']

test_corpus.iloc[6]['en_text']

"""**I'm bored** antes se traduc√≠a como **meeee bored**, as√≠ que hemos ganado en calidad.

Generar a partir de text_v2, text_v3 donde se pase text_v2 y se haga el spell_checker
"""

# eliminar nulos

df_train = test_corpus[~test_corpus.text_v5.isna()]
df_train